\chapter{Discussion}
The promise of IDI to allow three-dimensional, high resolution, element-specific imaging of nanoscale samples could not yet be fulfilled by the experimental implementation.

The estimated number of independent modes from the spectra seems lower than expected. However, one must keep in mind that the used regression scheme has a low sensitivity for higher $M$ values due to the diminishing gradient and might underestimate the number of modes. Nevertheless, this indicates that the number of modes is much higher than the "optimal" value for unpolarized light ($M=2$), resulting in severely reduced contrast. The amplitude of the intensity correlations observed in the foil samples is a factor of 2-4 less than previously measured by Inoue et al. and suggests more than 200 modes. This might be the combination of a longer pulse length due to less strict filtering of the shots (a different trade-off chosen with regards to peak contrast vs. noise reduction by averaging over more images) and the only partially by the regression corrected undersampling creating additional spatial modes.

The nanoparticle samples had much less prominent features according to the SAXS measurements than initially planned and compared to the structure factor for non-interacting spheres. Optimization of the preparation method leading to either higher concentrations of non-aggregating particles or to complete aggregation and the formation of quasi-crystals might lead to stronger features. Also, liquid injectors might be an option, even though, partially due to the tight focus, short Rayleigh length, and therefore difficult to achieve spatial overlap of the beam and sample, which would introduce additional complexity to the experimental setup.

The achieved alignment of the crystalline samples after determination of the mean orientation, and translational offset is still worse than the resolution of the reconstruction and the size of the expected Bragg peaks, resulting most likely in a reduction of the signal contrast through averaging of different pixel pairs not belonging to the same true $\vec{q}$. Additionally, the regression has only been performed on the average image; thus, variations over the scanning of the sample are not corrected. Mounting of the sample on the window might as well as the damage done during the measurement by the focused beam might have resulted in stress, slight bending of the thin glass stabilizing the crystal sheet, and stress in the lattice. Additionally, thermal motion of the gallium will have reduced the contrast.

Severe detector artifacts reduced the amount of usable data significantly, which, unfortunately, has not been noticed during the experiment - showing the necessity of better continuous online monitoring of the recorded data. The photon-counting applied to the raw images improves the fidelity of the focal images. Depending on the PSF of the detectors and sufficiently low photon counts, more sophisticated droplet schemes based on error minimization allowing for subpixel resolution, incorporating the scattering photons into the droplet algorithm or using a neural network trained on synthetic images could reduce the influence of noise and undersampling \cite{baumann2018,collaboration2014,schayck2020,sun2020}.

Even though the application of non-uniform FFT (NUFFT) based correlation estimators with sophisticated interpolation kernels, which have successfully been used in other fields, would require more memory, the reduction in under-sampling by making full use of the smaller $\vec{q}$ spacing at higher detector angles could make it a worthwhile extension of the used 3d reconstruction \cite{laguna1998,yang2008,chang2020}. Another possible avenue for improvement of the implementation might be to reconstruct only relevant parts of the reciprocal space, significantly reducing computational time and memory requirements. 

