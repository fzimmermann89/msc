\chapter{Simulations}
\label{chap:simulation}
To illustrate the working principle of IDI and to examine the Signal-to-Noise characteristics, the results of different simulations will be shown:\\
First, it is assumed that the object to be imaged consists of discrete emitters, each emitting monochromatic spherical waves with a randomly chosen phase, and the speckle image on a pixelated detector was simulated by the addition of scalar electric fields with random initial phase propagated to the detector and Poisson sampling in each pixel. In this time independent configuration, the speckle images of a single particle with randomly positioned emitters inside (approximating a single particle imaging setup), a focal volume filled with randomly positioned (non intersecting) hard spheres consisting of randomly positioned atoms (approximating, for example, many spherical nanoparticles imaged simultaneously) as well as a crystalline structure with emitters positioned within a lattice are simulated. In the first two cases, a small-angle regime was chosen, and the reconstruction was performed in 2D and as a 1D radial profile. For the crystalline structure, a realistic lattice constant in the same order of magnitude as the K$\alpha$ wavelength moves the reconstruction out of the small-angle regime, and a 3D reconstruction of the reciprocal space is performed. Additionally, in these simulations the effect of under-sampling is studied.\\
Secondly, to examine the influence of the finite fluorescence lifetime and pulse width, and the validity of approximating  this influence by the overlay of a discrete number of speckle images according to the estimated number of modes, a time-resolved simulation is performed.

Ultimately, the results of these simulations are used to estimate the parameters for an experimental setup.

\section{Time Independent Simulations}
In an infinite coherence time, stationary sources approximation, the simulation of the speckle pattern can be performed time independently as superposition of scalar electrical fields emitted with random phases $\phi_n$ from with $N$ excited atoms  at positions $r_n$ chosen randomly out of all possible atom positions inside the sample,
\begin{equation}
E(\vec{r})=\sum_n^N \frac{1}{\left|\vec{r}_n-\vec{r}\right|} e^{i(k_n{\left|\vec{r}_n-\vec{r}\right|}+\phi_n)} \,.
\end{equation} 
The simulation of the intensity $I=\left|E\right|^2$ at multiple discrete points $r$ on the simulated detector and Poisson sampling (with mean photon numbers according to the number of simulated excited atoms, distance  and simulated pixelsize), can be performed in parallel using GPU acceleration, resulting in a simple and fast to evaluate model.  To reduce the influence of the discrete sampling on the simulated speckle patterns, the simulation is performed at a higher and downsampled, such that each data point is the result of multiple calculations. 


\subsection{Detector Effects}
\label{sec:chargesharing}
To simulate the influence of charge-sharing and readout noise of the detector, an image degradation is performed: After Poisson sampling the simulated speckle image, for each photon a uniform random position within its pixel is chosen as the center of a Gaussian distribution with $\sigma$=0.1, pixel, similar to the size of the point spread function (PSF) for MPCCD detectors \cite{mpccd}.
The signal within each pixel caused by one photon is given by the integral over the Gaussian,
\begin{equation*}
	I(\Delta x,\Delta y)=\frac{1}{4} \left(\text{erf}\left(\frac{\sfrac{1}{2}-\Delta x}{\sqrt{2}
		\sigma}\right)+\text{erf}\left(\frac{\sfrac{1}{2}+\Delta x}{\sqrt{2} \sigma}\right)\right) \left(\text{erf}\left(\frac{\sfrac{1}{2}-\Delta y}{\sqrt{2}
		\sigma}\right)+\text{erf}\left(\frac{\sfrac{1}{2}+\Delta y}{\sqrt{2} \sigma}\right)\right)
\end{equation*}
with $\sigma$ the standard deviation of the Gaussian in pixels and $\Delta x$, $\Delta y$ the distance of the pixel to the drawn photon center. Afterwards, a Gaussian readout noise is added. The effect of this degradation on the spectrum is illustrated in \fref{fig:degrad}.

\subsection{Autocorrelation}
The numerator of $g^2$ is the autocorrelation of the recorded intensities $I$ which can be calculated as follows:
Let $x$ and $y$ describe positions in the image and $z(x,y)$ the distance to the sample, then 
with $\vec{q}(x,y) = \sfrac{[x y z]}{\sqrt{x ^ 2 + y ^ 2 + z^2(x,y)}}$
\begin{equation}
\overline{AC}(\Delta \vec{q}) = 
\frac{1}{N_p} \sum_{\vec{q}} AC(\vec{q},\vec{q}+\Delta \vec{q})=
\frac{1}{N_p} \quad \sum^{	\crampedclap{\sfrac{N_x}{2},\sfrac{N_y}{2}}}_{
	\crampedclap{
		\begin{subarray}{l}
		x_1=\shortminus\sfrac{N_x}{2}\\
		y_1=\shortminus\sfrac{N_y}{2}
		\end{subarray}
	}}\quad
\sum_{\crampedclap{\qquad\qquad\qquad\qquad
		\begin{subarray}{l}
			x_2,y_2 \text{ where} \\ \lfloor q(x_1,y_1)-q(x_2,y_2)\rfloor=\lfloor\Delta\vec{q}\rfloor
		\end{subarray}}}
	I(x_1,y_1),I(x_2,y_2)
	\label{eq:correlation}
\end{equation}
with $N_p$ the total number of pairs of points on the detector $[x_1 y_1],[x_2,y_2]$  fulfilling the condition.

Making use of the (circular) convolution theorem of the discrete Fourier transformation,
\begin{equation}
\operatorname{DFT}_N^{-1}\left[\operatorname{DFT}_N x \cdot \operatorname{DFT}_N x\right]_n=\sum_{\ell=0}^{N-1} y_{\ell} \cdot x_{(n-\ell) \bmod N}
\end{equation}
which after zero-extending the signals x,y to $2N$ reduces to the regular convolution, the autocorrelation of the image can be written as 
\begin{equation}
\sum_{\ell=0}^{N-1} I_{\ell} \cdot I_{(\ell+q) \bmod 2N}=\operatorname{DFT}_{2N}^{-1}\left[\left|\operatorname{DFT}_{2N} {x}\right|^2\right]\equiv \operatorname{AC}(I)
\label{eq:fftcorrelation}
\end{equation} 
as $I\in \Re$ and $I_{n\geq N}$=0, greatly reducing the computational complexity \cite{oppenheim2009}. In wide-angle setups with non constant $q_z$ and changing angle covered by each pixel a resampling of the recorded images to a regular, three dimensional $q$-grid is performed by assigning each pixel of the image its nearest neighbor in the new grid, averaging over all pixels having the same nearest neighbor and placing the averages at these grid positions without any additional interpolation. 

The number of correlation pairs $N_p$ which contribute to the sum in \fref{eq:fftcorrelation} for each $q$ can be found by performing the same operations with a constant image (of the same shape) $M$ of all ones, leading to the numerator of $g^2$ being $\operatorname{AC}(I)/\operatorname{AC}(M)$ \cite{oppenheim2009,butz2015,nion2008}.

\subsection{Normalization}
\label{sec:normal}
In the definition of $g^2$ (\fref{eq:g2}), normalization is performed by the product of the \textit{expectation values} of the intensities at the two locations. As these expectation values are influenced by the sample, the geometry, and experimental imperfections, some considerations how to implement this normalization have to be made for calculating $g^2(\Delta\vec{q})$ from measured or simulated fluorescence speckle patterns. 

If only a single image is taken, the expectation value for each pixel has to be roughly approximated. This can be done by assuming an equal value for all pixels (and estimating by the mean), which will lead to significant artifacts if, for example, due to the geometry, outside pixels have a much lower true expectation value than pixels in the center of the recorded image.  
For masked image registration and similar tasks, the cross correlation is commonly normalized by the following (simplified to the relevant case of the autocorrelation) divisor \cite{padfield2010}:
\begin{equation}
\frac{\mathit{corr}(\mathit{mask},\mathit{image}) \mathit{corr}(\mathit{image},\mathit{mask})}{\mathit{corr}(\mathit{mask},\mathit{mask})} \,,
\end{equation} which would correspond to estimation of the expectation values  by the mean over all pixels that form pixel pairs contributing to the value in the reconstruction at a particular $\Delta \vec{q}$, giving an slightly different weighting to the pixels than first first calculating $g^2$ normalized by the true expectation values for each pair and afterwards averaging over the pairs would give.

If multiple images are recorded or simulated, the expectation value for each pixel can be approximated by its mean over many realizations, i.e. images. To account for fluctuations in the exciting FEL pulse and resulting fluctuations of the fluorescence intensity, under the assumption that the distribution over pixel intensities and FEL intensities are uncorrelated, the expectation value of photon detection probability of each pixel in each shot can be factorized into the product of the probability to detect a photon in a particular pixel and the probability to detect a photon in a shot. Thus, the effect of the FEL intensity fluctuations can be suppressed by normalizing each image by the total intensity, and the uneven probabilities for different pixels by normalizing each pixel by its mean over all images. This normalization scheme will be used for all reconstructions in this thesis.
For brevity, a detailed comparison of the different normalization approaches is not shown. Still, the successful reconstructions of the reciprocal space, for both wide and small angle simulations with varying amounts of noise and simulated photon numbers, without any pedestal ($g^2(\infty)\not\approx 1$) or strong artifacts throughout this chapter indicate that the applied normalization is sufficient.  



\subsection{Photon counting}
As the relevant signal for the correlation analysis is the presence of fluorescence photons, but charge sharing and readout noise of the detector as well as the presence of photons caused by air scattering and straylight degrade this signal, different approaches  of preprocessing to reduce this degradation are compared:

\begin{itemize}[nosep]
	\item Using the raw signal
	\item Using the raw signal after applying a noise threshold.
	\item Discretization by closest possible combination of signal and scattering photons for each pixel
	\item Discretization by maximum likelihood
	\item Using a droplet algorithm
\end{itemize}

The noise thresholding is done with a threshold of three times the standard deviation of the detector noise. Considering possible combinations of signal and scattering photons by finding  $nE_{fluorescence}+mE_{excitation}$ with integer $n$, $m$ closest to the observed value and only using the signal photon number $n$ for the correlation could reduce the influence of scattering as well as charge sharing, but does not account for the different probabilities of signal and scattering photons. If the detector noise level, the point spread function due to charge sharing as well as the distribution of signal and scattering is known or can be estimated, a Bayesian classifier can be trained on synthetic data, which returns for each observed value on the detector the number of signal photons with the highest probability causing this value (\fref{fig:probs}) and the associated decision boundaries. Depending on the apriori probabilities and the detector characteristics, this maximum likelihood solution can differ from the closest combination. For comparison, the PSANA Photon algorithm, a droplet algorithm considering only the signal photons is used \cite{psana}. The disadvantage of most droplet algorithms is the more obscure effect on the correlations, as they influence the position of photons and usually consider only a single energy or require a much higher computational effort.  

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\linewidth]{images/sharing.pdf}
		\caption{True histogram and degradation}
		\label{fig:degrad}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\linewidth]{images/probs.pdf}
		\caption{Probabilities and decision boundaries}
		\label{fig:probs}
	\end{subfigure}
	
	\caption[Histogram, probabilities and decision boundaries for the photon number under the influence of charge sharing and noise]{For a Poisson distributed signal with mean 0.01 photons/pixel, a Poisson distributed scattering with mean 0.001 photons/pixel and an photon energy of 1.3 times the energy of a signal photon, a charge sharing PSF with $\sigma$ 0.1 pixel and a Gaussian noise with $\sigma$ 0.05 photons, the simulated histogram is shown on the left. Detector noise, charge sharing scattering photons degrade the histogram. The probabilities of an observed energy being caused by a certain number of signal photons is shown on the right, the dashed lines mark the decision boundaries for discrete signal photons of an Bayesian classifier.} 
\end{figure}


\begin{figure}
	\centering
	\begin{subfigure}[b]{0.85\textwidth}
		\includegraphics[width=\linewidth]{images/photonreconimg.pdf}	
		\caption{Center part of the reconstructions}
		\label{fig:photonreconimg}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.95\textwidth}
		\includegraphics[width=\linewidth]{images/photonrecon.pdf}
		\caption{Line profiles}
		\label{fig:photonrecon}
	\end{subfigure}	
	
	\caption[Comparison of different photon counting methods for intensity correlation calculations]{Comparison of different photon counting methods for intensity correlation calculations: In \textbf{(a)} the center of the 2d results of the correlation for a simulated grating under two conditions are shown (high signal and low signal, parameters as described in the main text). In  \textbf{(b)}, for three different conditions (from left to right: high signal, low signal and high noise) line profiles are shown. Using the raw signal or simple thresholding overestimates the correlation between neighboring pixels, the PSANA droplet algorithm underestimates it compared to the ground truth. The maximum likelihood method and the "closest combination approach" reproduce the ground truth signal sufficiently well. For detailed quantitative results see also \fref{tab:photonrecon}.}
\end{figure}
\begin{table}
	\caption[Ratio of $g^2-1$ of different photon counting schemes to ground truth value]{Ratio of $g^2-1$ of different photon counting schemes to ground truth value  for $\Delta$=1\, pixel/first maximum. \textit{Closest} is \textit{Combination} without considering Noise photons, giving good results in the low noise cases.}
	\label{tab:photonrecon}
	
	\begin{adjustwidth}{-1em}{-2em}	
		\small
		\begin{tabular}{lllllll}
			\toprule
			&        Raw &       Threshold &         Combination &      Closest &         MaxL &          PSANA \\
			\midrule
			Low Signal  &  12 / 0.59 &  6.9 / 0.64 &  0.62 / 0.79 &  0.88 / 0.68 &  0.57 / 0.81 &    -0.5 / 0.66 \\
			High Signal &  2.4 / 1.1 &   1.8 / 1.0 &  0.72 / 0.82 &   1.1 / 0.96 &   1.0 / 0.95 &  -0.026 / 0.79 \\
			High Noise  &  9.3 / 0.3 &  8.1 / 0.39 &   1.0 / 0.58 &   1.3 / 0.46 &  0.44 / 0.54 &   -0.37 / 0.45 \\
			\bottomrule
		\end{tabular}
	\end{adjustwidth}
\end{table}

To evaluate the different approaches, for a grating with pitch 50\,nm, line width 20\,nm and 100\,nm thickness in a 400\,nm (FWHM) focus, resulting in $10^7$ excited atoms emitting at 10\,keV and a 1024x1024 pixel detector with 50\,um pixel size in 50\,cm distance, independent, 2000 florescence images are simulated. Three different cases are considered: With high signal ($10^5 $photons/image), low signal ($10^4$ photons/image), both having $10^3$ photons at 13\,keV as scattering noise and 500\,ev (gaussian) detector noise as a "high noise" case  with 2.5\,times the scattering and 2\,times the detector noise  ($10^4$ signal photons/image). Charge sharing is in all cases simulated with $\sigma$=5\,um.  For each photon counting approach, normalized intensity correlations are calculated and compared to a reconstruction without noise or charge sharing as ground truth. For the first two cases, the center part of the correlation is show in \fref{fig:photonreconimg}, line profiles through the center and the first maximum for all cases are shown in \fref{fig:photonrecon}. In the different reconstructions, two effects are visible: In comparison with the ground truth, without any correction $g^2$ for low $q$ is be overestimated due to the correlation of neighboring pixels by charge sharing and underestimated for the first maximum (see \fref{tab:photonrecon} for detailed values). This can be slightly reduced by thresholding, and greatly reduced by rounding to the closest combination as well as by the maximum likelihood classification. The PSANA droplet algorithm seems to join more neighboring photons than necessary, resulting in an incorrect negative correlation between neighboring pixels.  

\subsection{Accessible Reciprocal Space}

	As IDI is based on $g^2(\Delta \vec{q})$, for a set of accessible $\vec{q}$ determined by the experimental geometry ($k$, detector size and distance), IDI can give information about $S(\vec{q})$ at higher $\left|\vec{q}\right|$ than a traditional scattering setup, increasing the numerical aperture (in a small angle regime up to a factor of two) and could in theory be used to achieve a higher resolution. In practice, for most samples the resolution will be limited by the (approximate) $q^{-4}$ dependence of the structure factor due to Porod's law and the SNR, thus only increasing the resolution for samples with strong features at high $q$.
	
	Compared to CDI, which measures $\vec{q}$ on the Ewald sphere and gives only limited $q_z$ information, IDI with a flat detector can give access to a three dimensional volume in reciprocal space, as shown in \fref{fig:accesibleq} and, with greater $q_z$ coverage the greater the curvature of the Ewald sphere is. This, for example, gives access to multiple Bragg peaks in a single crystal experiment as shown in \fref{fig:accesiblebraggq}. 
		

\subsection{Detector Size and SNR}
	
	To asses the influence of the number of pixels of an detector on the SNR, a simulation for a 1\,um thick Copper foil in an 100\,nm FWHM focus is performed. The detector size was varied from 64x64 to 3072x3072 pixels and always placed at the same distance of 1\,m, keeping the mean photon count per pixel constant. The number of correlation averaged over in the reconstruction increases linear with the number of pixels. For the SNR calculations, the signal is defined as peak intensity, the noise as the standard deviation over independent simulations.  As show in \fref{fig:SNRdetsize}, under these conditions, the SNR is proportional to the square root of the number of pixels of the detector. 



\begin{figure}
	\centering
	\begin{tabular}[t]{cc}
		\begin{tabular}{c}
		\smallskip
		\begin{subfigure}[t]{0.42\textwidth}
			\centering
			\includegraphics[width=0.9\textwidth]{images/accessibleq2.png}
			\caption{Accessible reciprocal space compared to CDI }
			\label{fig:accesibleq}
		\end{subfigure}\\
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[width=0.9\textwidth]{images/accessibleq.png}
			\caption{Accessible reciprocal space with Bragg peaks}
			\label{fig:accesiblebraggq}
		\end{subfigure}
	\end{tabular}
		& 
	\begin{subfigure}{0.52\textwidth}
	\centering
	\includegraphics[width=\linewidth]{images/pairsnoise.pdf}
	\caption{Correlation pairs and noise dependence on $\vec{q}$ } 
\end{subfigure}\\
	\end{tabular}
	\caption[Accessible reciprocal space]{The accessible reciprocal space with an 2048x2048\,pixel (100\,um pixelsize) at 12.5\,cm distance and 9.2\,keV is shown in a) and b). In a), the surface of the Ewald sphere accessible in a diffraction experiment is shown in green for comparison. IDI allows a reconstruction of a three dimensional volume. In b), the position of GaAs Bragg peaks as determined by finding local maxima (>4 standard deviations) in the reconstruction is shown inside this volume. As the accessible $q_z$ is depended on $q_x$/$q_y$ and low near the limits of the latter, a precise alignment of the detector with regard to the lattice planes is necessary to be able to image the maximal number of peaks. Using a square, centered, planar detector with uniform pixelsize, the number of correlation pairs for resulting in the same $\vec{q}$ depends on $\vec{q}$ as shown in on the left of c) for a $q_z=0$ slice. The noise (calculated as the standard deviation over 100 independent simulations) is therefore also non-uniform, as shown on the right of c).}
	
\end{figure}



\subsection{Number of Images}
As shown in \fref{fig:SNRNimages} if only considering shot and phase noise (uncorrelated between different  images), the SNR scales with the square root of the number of images.  This can be used to estimate the number of shots necessary to achieve a SNR greater than three to be able to experimentally verify IDI as an imaging method. Systematic noise sources causing correlated noise does not follow this relation and should be minimized, e.g. by masking out affected areas of the detector.
 
\subsection{Number of Modes}
To Illustrate the effect of the number of modes on the signal and the SNR, simulations are performed by averaging the intensity over $M$ realizations of the random phases.  In the results of the simulation of a crystall sample, (\fref{fig:modes}) the predicted $1/M$ scaling of the signal can be seen. Furthermore, as expected, in \fref{fig:SNRNimages}, the SNR shows the same scaling.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\linewidth]{images/modes_signal.pdf}
		\caption{Signal dependence on the number of modes\\ ${}$}
		\label{fig:modes}
	\end{subfigure}
	\hspace{0.02\textwidth}
	\begin{subfigure}[b]{0.45\textwidth}
	\includegraphics[width=\linewidth]{images/SNRNimagesgrating.pdf}
	\caption{SNR dependence on the number of modes and images}
	\label{fig:SNRNimages}
\end{subfigure}
\caption[SNR dependence on the number of modes and images]{For a simulated crystal
	(sc-structure, 200x200x500 atoms, 5\,\AA\,lattice constant, 2048x2048 detector with 100\,um pixelsize at 50\,cm, $10^4$ photons/image on the detector, 10\,keV) dependence of the signal (height of the peak) on the number of simulated modes shows approximately an $1/M$ relation.  The inset shows individual line profiles through a Bragg peak in the reconstruction. In (b), for a simulated Nickel grating (80\,nm pitch, 40\,nm thickness in a 200\,nm focus, 1024x1024 detector, 70\,cm) the dependence of the SNR on the number of images averaged over and modes fits the $\sqrt{N}/M$ relation, which is indicated by dashed lines.}
\end{figure}

\subsection{Undersampling and  Sample Size}


A simulation of a cubic single crystal with a simple cubic lattice of varying size (from 20$^3$ to 200$^3$\,atoms) is performed. The lattice constant is chosen as 5.7\,\AA, the fluorescence energy as 8\,keV. The simulated detector has 1024x1024 50\,µm sized pixels and is placed 8\,cm from the  sample. 
The simulation of the fluorescence patterns is performed with 4x4 oversampling (4096x4096 pixel) and rebinning to the detector size. Only a single coherence mode is simulated. The number of photons emitted by the sample in 4$\pi$ is chosen as half the number of atoms in the sample. Hence, the mean number of photons per pixel is (especially for small crystal sizes) very small, but at the upper bound of an achievable photon yield in an experiment.

The peak signal-to-noise ratio is calculated by simulating 2000 independent images  and taking the mean intensity at the visible Bragg peaks positions as signal and the standard deviations at those positions over the independent simulations as noise, resulting in an estimated of the peak SNR of a single image.
Due to the low photon numbers, the Poisson noise is dominating the noise characteristic and with an increase in atoms in the focus, the SNR increases linear (as shown in \fref{fig:SNRNatoms}), up to the point where a the peaks are no longer fully sampled and the signal decreases linear with a further increase in the number of atoms in the crystal, resulting in a nearly constant peak SNR.
If instead of considering the peak value of the reconstruction as signal, the three dimensional integral over the Bragg peak would be considered as signal, the decreasing width of the Bragg peak with increasing size of the crystal would result in a nearly constant SNR under these low photon count conditions. 

This results suggests, that for an experimental verification using Bragg peaks, the excited volume of the crystal should be as large as possible without undersampling - for example choosing a thin single crystal and an appropriately tight focal volume.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\linewidth]{images/SNRNatoms.pdf}
		\caption{ SNR dependence on crystal size}
		\label{fig:SNRNatoms}
	\end{subfigure}
	\hspace{0.02\textwidth}
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\linewidth]{images/detsize.pdf}
		\caption{SNR dependence on detector size}
		\label{fig:SNRdetsize}
	\end{subfigure}
\caption[SNR dependence on crystal size and detector size]{SNR dependence on the crystal size (a): As the crystal size increases, the SNR increases linear (as shown by the linear regression) with the number of atoms in the sample as long as the Bragg peaks is sufficiently sampled (blue points), in undersampling conditions the signal strength decreases linear with the number of atoms resulting in a near constant SNR (right side of the dashed line). The error bars show the standard deviation of the SNR calculated for the 12 visible Bragg peaks. For details on the simulation parameters, see main text. The SNR dependence on the detector size (b) shows a linear relationship between the square root of the number of pixels and the SNR.}

\end{figure}


%\subsection{Random Orientations}
%In an atomic resolution IDI experiment using a crystalline sample, there can be random orientations of the sample structure between each different shot if a powder is used as a sample.
%If a superlattice structure is used, it can be possible to limit the random orientations to orientations within a plane.
%The random orientations cause randomly oriented Bragg peaks and a significantly lower SNR as shown in \fref{fig:orientation}.  To have the best chance of experimental verification, a sample with static orientations between all shots used in the reconstruction is therefore beneficial.
%For random orientations in a plane and  imaging Bragg peaks of, it is possible to do an angular correlation of each reconstructed image and increase the SNR (as shown in \fref{fig:polarcor}), similar to a method in correlated X-ray scattering \cite{mendez2016}.




\subsection{Multiple Samples}
To decide if having more than one spherical sample in the focus is advantageous, an additional simulation is performed:
Multiple spheres inside the focal volume, placed randomly but ensuring a minimum distance between neighboring spheres larger than the diameter plus twice the thickness of an additional layer of non-fluorescing organic dispersion agent capping the spheres with no other interaction are simulated.  The number of particles is varied, ranging from a single sphere over multiple spheres to a Poisson Sphere Distribution as a random placement of particles with the minimal allowed spacing and mean a volume fraction of approx. 25\%\footnote{without the buffer layer, the volume fraction would be around 50\%, significantly smaller than the densest possible packing}. 
The structure factor of this ensemble of spherical particles is determined by three factors: The structure factor of the focus, the structure factor of points following an Poisson Sphere Distribution and the structure factor of a single sphere.(see \fref{fig:multisphere1}). As the number of spheres is increased, the influence of the focal volume increases and the distribution of the spheres's centers increases (see  \fref{fig:multisphere3})
For spheres with 20\, nm radius, a spacing layer of 5\,nm around each sphere, with  50000 excited atoms per sphere on average, a focus of 200nm (FWHM), the fluorescence on a 1024x1024 pixel (pixelsize 100\,um) detector placed 30\,cm is simulated. In this geometry, assuming constant distance to the sample for each detector pixel, approximately 1\% of the emitted photons reach the detector. 
For each number of spheres 5000 images are used for an IDI reconstruction (see  \fref{fig:multisphere2}).
As the number of photons sphere is kept constant, opposing effects occur:  With increasing number of particles, more are photons recorded and the Poisson noise is reduced, but the signal strength in low scattering angles is decreased as the structure factor changes from a single sphere to a hard-sphere model. Therefore, for the chosen simulation parameters an optimum in the detectebility of a correlation effect can be found for a medium number of samples in the focus.


As the number of particles inside the focus is increased, in the low $q$ region of the IDI reconstruction, the focal volume becomes more visible. For high numbers of particles, the structure factor of the distribution of the centers of the particles causes a reduction in the scattering factor in the low $q$ region. For low numbers of particles inside the focus, the Poisson noise casued by the low photon numbers dominates the error.


 
\begin{figure}
	\centering
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\linewidth]{images/multisphere1.pdf}
		\caption{Structure Factors\\$ $}
		\label{fig:multisphere1}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\linewidth]{images/multisphere3.pdf}
		\caption{Structure Factor of Multiple Spheres}
		\label{fig:multisphere3}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\linewidth]{images/multisphere2.pdf}
		\caption{IDI Reconstruction \\ $ $}
		\label{fig:multisphere2}
	\end{subfigure}
\caption[Structure factors and reconstructions for multiple spherical samples]{Structure factors of a sphere with 20\,nm radius, a 200\,nm Gaussian focus and a random distribution of points at least 50\,nm apart (a), the structure factor of different numbers of hard 20\,nm radius spheres with an additional 5\,nm separating layer on each sphere inside the focal volume (b), and the result of an IDI simulation assuming iron fluorescence, a mean of $5*10^4$ excited atoms per sphere, a 1024x1024 pixel (100\,um pixelsize) detector in a distance of 30\,cm (resulting in approximately 1\% of the emitted photons being captured), and using 5000 images (c). The shaded area is the standard error of the mean over images, the markers show the discrete $q$ steps in the reconstruction.}

\end{figure}

\section{Time Dependent Simulations}
\label{sec:timedependend}
Each of the $N$ atoms is assigned an emitting time $t_{n}$ chosen according to the excitation pulse shape and its position. Starting from this emitting time, the atom emits an exponential decaying field with a decay time chosen to match the lifetime. 



For each discrete pixel on the simulated detector, for each atom the distance $d_n$, the arrival time $t'_n=t_n+d_n/c$ of each atom's initial radiance, and its time independent complex field $E_n=\frac{1}{d} e^{ikd_n+\phi_n}$ with initial random phase $\phi$ is calculated.
The time dependent E field is the summation over the decaying field of all atoms,
\begin{equation}
E(t)=\sum_{n=1}^N  E_n \Theta(t'_n  - t) * e^{-(t-t'_n )/\tau}
\label{eq:tdsum}
\end{equation}
and the simulated intensity the time integral over the magnitude squared of the E-field,
\begin{equation}
I=\int_0^\infty \left| E(t) \right|^2 .
\label{eq:tdint}
\end{equation}
To efficiently solve this integral for each detector pixel, it can be split into N parts with a constant number atoms which radiations have already arrived, and each of those parts can be solved analytically. For this, first all atoms are sorted by the arrival time at the pixel. At each arrival time $t'_n$, the sum in \fref{eq:tdsum} gets a new term and the field is calculated as
\begin{equation}
E(t_n)=E(t_{n-1})*e^{\sfrac{t_{n-1}-t_n}{\tau}}+E_n
\end{equation}
 This can be done with a parallel inclusive scan, as shown in \fref{algo:td}. This gives the supports for the integral, as show in \fref{fig:tdplot}, which can now be solved as
\begin{align}
	I&=\int_0^\infty \left| E(t) \right|^2 = \sum_{n=1}^{N-1} \int_0^{t_{n+1}-t_n} \left|E(t_n)\right|^2 e^{-2t/\tau} \dif t +\int_0^\infty \left| E(t_N)\right|^2 e^{-2t/\tau} \dif t \\
	 &=  \frac{\tau}{2}  \left|E(t_N)\right|^2 -  \frac{\tau}{2}\sum_{n=1}^{N-1} \left|E(t_n)\right|^2 (e^{-2 (t_{n+1}-t_n)/\tau} -1 ) 
\end{align}
This procedure is efficient in regards of discrete time steps that need to be calculated and can easily be run on a GPU.

\subsection{Influence of the Pulse Length}
For a sphere with 10\,nm radius consisting of $2*10^5$ atoms emitting 6.4\,keV fluorescence captured by an 256x256@50\,um detector in 20\,cm distance, the speckle strength (calculated as the standard deviation of the speckle pattern over the mean) of a series of simulations with different decay times $\tau$ of the emission and different FWHM of the exciting Gaussian pulse are shown in \fref{fig:pulsedecay}.  These follow the $1/\sqrt{erfcx(2\sigma/\tau})$ relation as predicted by the estimation of the number of modes in \fref{sec:specklecontrast}.  For each simulation, a reconstruction can be performed, resulting in radial profiles as shown for one $\tau$ in \fref{fig:tdpshere}. The visibility of the reconstructions (\fref{fig:tdpshere_visrecons}) shows the for long pulses a reciprocal relationship.

\subsection{Influence of the Sample Thickness}
To investigate the influence of the sample thickness, the speckle strength in a simulation is shown in \fref{fig:thickness}. In this simulation, a constant number ($10^6$) of 8\,keV emitters are placed inside a 200\,nm x 200\,nm (FWHM) Gaussian volume with varying thickness.  The axis of varying thickness is always set perpendicular to the detector, and the viewing angle $\alpha$ does not change the overall volume in which the emitters are placed, ensuring a constant speckle size. Therefore, the change in SNR is only caused by the finite coherence time $\tau$=1\,fs and the 1\,fs FWHM Gaussian from which the emission times are sampled, not by a change of the sampling conditions. The 64x64 pixel (pixelsize 100\,um) detector is placed in 1\,m distance. The simulation is performed with 4x oversampling and rebinning in both directions.  
These simulations show that in high angles, the limited coherence length of the fluorescence reduces the speckle SNR for thick samples, whereas in small angles, the sample thickness does not influence the SNR: In the 0° limit, the thickness is in beam direction and the position of an emitter along this axis does not change the arrival time of its contribution the speckle pattern, as the path length difference $\Delta$ caused by the sample thickness $t$ is (with the transversal distance between the sample and detector $z$ and the lateral distance $x$)
\begin{equation}
	\Delta=z+\sqrt{z^2+x^2}-\sqrt{(z+t)^2+x^2} \approx(1-\cos (\alpha)) t=\frac{k^2}{\left|\vec{q}\right|^2}t \,.
\end{equation}


 As a thicker sample gives more photons and therefore less Poisson noise, but a higher number of temporal and spatial modes, thus a lower expected lower speckle visibility, a trade-off has to be made.


\begin{figure}
	   \centering
		\includegraphics[width=0.5\linewidth]{images/tdplot.pdf}
	\caption[Integration in time dependent IDI simulation]{To illustrate the integration in the time dependent IDI simulation, the (normalized) scalar field at one point of the detector created by 10 emitting atoms with $\tau$=1\,fs and a pulse FWHM of 2\,fs is shown. The solid colors show each atom's field, the line plot the (phase correct) sum. The simplify the calculations, only the time points marked with stars are calculated. To solve the integral \fref{eq:tdint}, it's sufficient to calculated the squared magnitude at those points and to do a piecewise integration from each star-shaped marker the next dot-shaped marker. The color wheel in the top illustrates the phase encoding in the plot.}
	\label{fig:tdplot}
\end{figure}


\begin{figure}
\centering
	
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\linewidth]{images/timedependent_1.pdf}
		\caption{Speckle Strength for different pulse FWHM and decay times $\tau$}
		\label{fig:pulsedecay}
	\end{subfigure}
	\hspace{0.1cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/tdsphere.pdf}
		\caption{Reconstructed radial profiles at different pulse FWHM and fixed $\tau = 0.1$\,fs}
		\label{fig:tdpshere}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/timedependent_2.pdf}
		\caption{Visibility of the reconstruction for different pulse FWHM and decay times $\tau$}
		\label{fig:tdpshere_visrecons}
	\end{subfigure}
	\hspace{0.1cm}
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\linewidth]{images/thickness.pdf}
		\caption{Speckle strength for different sample thicknesses and angles}
		\label{fig:thickness}
	\end{subfigure}	
	\caption[Speckle strength and signal visibility in time dependent simulation]{ In \textbf{(a)} the strength of the simulated speckle pattern for different pulse length and decay times $\tau$ of a spherical object (radius 10\,nm) is compared with the theoretical dependence on the ratio of pulse length and $\tau$, showing good agreement. 
	In \textbf{(b)} exemplary radial profiles of the reconstruction from 50 of those patterns are shown for one fixed $\tau$. Those reconstructions are used to determine the signal visibility as quantification of the  dependence of the signal strength on the pulse width in \textbf{(c)}. For long pulses, the reciprocal dependence on the pulse length is visible.
	In \textbf{(d)}  the influence of sample thickness $t$ on the speckle strength under different angles is show (using the mean of 5 independent simulations and the standard deviation as errors). The dashed lines are the approximation $s(t,\alpha)=\frac{s(t\rightarrow0\text{um}, \alpha= 0)}{1+\left(1-\cos{\alpha}\right)\sfrac{t}{2.35\text{um}}}$. For small angles, the sample thickness does not influence the speckle strength.}
\end{figure}

%$y=c/\left(1+(\sfrac{{4})\right)$

\clearpage
\section{Summary and Discussion of the Implications for an Experiment}

The insights about the influence of different numbers of spherical samples inside the focus will guide the preparation of nanoparticle samples: By depositing the nanoparticles in a polymer matrix at a concentration that is on the one hand high enough to make use of the gain in signal photons but on the other hand low enough not to lose contrast in the form factor.


The simulation of different samples and realistic conditions allows to estimate the number of images necessary for a successful IDI measurement: The result of a simulation for a thick 5\,um Copper foil, rotated 45° with respect to the beam (200\,nm focus FWHM, 10\,fs pulse FWHM), emitting only at a single energy line and polarisation, measured with a detector with 50\,um pixel size (charge sharing $\sigma$=5\,um) placed in a distance of 1\,m perpendicular to the beam is shown in \fref{fig:simfoil}. Even though the under-sampling in the horizontal direction causes a significant reduction in signal strength, 50 images suffice in the simulation to get an acceptable SNR (>3), leading to the estimation that, even with the additional reduction in contrast due to multiple emission lines and polarization (combined a factor <4), less than 1000 images would suffice for a replication of the previously published results \cite{nakumura2020}.

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{images/sim_foil5umCu_shared.pdf}
	\caption[Simulation of a metal foil with similar parameters as used in the experiment]{Simulation result for a metal foil placed in a 200\,nm focus viewed perpendicular to the incoming FEL beam. No correction for charge sharing is applied. The center pixel is masked, as $g^2(0)-1\approx 
		\frac{1}{M}+\frac{1}{\mu}$ is dominated by the mean count and does 
		not carry spatial information.}
	\label{fig:simfoil}
\end{figure}

The agreement between the time-dependent simulation with the approximation of the number of modes overlaid in the measurement allows to simplify the simulation of samples with many excited atoms (>$10^6$) and long pulse lengths, as the time-independent simulation is, despite the applied optimizations, still much faster to perform.

The most relevant implications for imaging crystals are: First, it is beneficial to use a single crystal compared to a powder to allow averaging over many shots with Bragg peaks at the same positions and there is a trade-off between the number of signal photons, undersampling, and loss of speckle contrast due to the path length difference caused by the sample thickness, suggesting to use few micrometer thick samples and a tight focus of a few hundred nanometers FWHM. Second, the orientation of the detector with regards to the sample has to be aligned reliably, as the reconstructed reciprocal volume is large compared to the size of the expected Bragg peaks, and additionally, the coverage in $q_z$ direction is finite (and depends on the detector position, size and masks applied) -- bad alignment could therefore lead to fewer potential Bragg positions being inside the reconstructed reciprocal space than optimal. For GaAs single crystals and a centered octal-MPCCD detector as available at SACLA, the sample-detector distance has to be 12-15\,cm and the alignment can be off by at most 1°. For crystals with larger distances in reciprocal space between the Bragg peaks (smaller lattice constant or different extinction rules), the distances would have to be closer accordingly, resulting in a smaller FOV for the same detector, and thus may limit the SNR due to undersampling.

According to the comparison of the different photon-counting methods, discretizing the recorded detector values by finding appropriate thresholds for the different numbers of signal photons reduces detector effects and should be used to analyze experimental data. Nevertheless, directly neighboring pixels may still be influenced, and their calculated correlations should not be considered trustworthy for determining the structure factor.







